{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7687ee4-dd00-4f18-88b8-bceacd766f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit langchain langchain_community langchain_core python-dotenv langchain-huggingface langchain-qdrant langchain-ollama unstructured[pdf] onnx==1.16.1 qdrant-client ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89354ca8-2913-4619-bdaa-9b13023d2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors stuff\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "# Setup Of Embeddings\n",
    "\n",
    "qdrant_url = \"http://localhost:6333\"\n",
    "collection_name: str = \"vector_db\"\n",
    "\n",
    "embeddingModel = HuggingFaceBgeEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-en\",\n",
    "            model_kwargs={\"device\": \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "        )\n",
    "\n",
    "def create_embeddings(pdf_path: str):\n",
    "        \"\"\"\n",
    "        Processes the PDF, creates embeddings, and stores them in Qdrant.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): The file path to the PDF document.\n",
    "\n",
    "        Returns:\n",
    "            str: Success message upon completion.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"The file {pdf_path} does not exist.\")\n",
    "\n",
    "        # Load and preprocess the document\n",
    "        loader = UnstructuredPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        if not docs:\n",
    "            raise ValueError(\"No documents were loaded from the PDF.\")\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=250\n",
    "        )\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        if not splits:\n",
    "            raise ValueError(\"No text chunks were created from the documents.\")\n",
    "\n",
    "        # Create and store embeddings in Qdrant\n",
    "        try:\n",
    "            qdrant = Qdrant.from_documents(\n",
    "                splits,\n",
    "                embeddingModel,\n",
    "                url=qdrant_url,\n",
    "                prefer_grpc=False,\n",
    "                collection_name=collection_name,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise ConnectionError(f\"Failed to connect to Qdrant: {e}\")\n",
    "\n",
    "        return \"âœ… Vector DB Successfully Created and Stored in Qdrant!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1dacf1-6886-4a86-8f5b-25377c5ee741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and store the embeddings for given pdf path\n",
    "\n",
    "create_embeddings(\"stomach-cancer.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb1d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qdrant client setup for exctracting context\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"http://localhost:6333\",\n",
    "    prefer_grpc=False\n",
    ")\n",
    "\n",
    "db = QdrantVectorStore(\n",
    "    client=client,\n",
    "    embedding=embeddingModel,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc70a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. keep the answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Ask the Question: \")\n",
    "results = rag_chain.invoke({\"input\": question})\n",
    "\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93008b-f39e-46ad-b2ac-d508670d0868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
